import java.io.FileWriter
import java.sql.{Connection, DriverManager, ResultSet}

// Function to send email notifications via a shell script with first email in To and others in CC
def sendEmailNotification(alertCode: String, message: String, emailAddresses: String, business: String): Unit = {
  val shellScriptPath = "/path/to/email_notification.sh"  // Update with actual path to script
  val emailList = emailAddresses.split(",").map(_.trim)

  if (emailList.nonEmpty) {
    val toEmail = emailList(0)
    val ccEmails = emailList.drop(1).mkString(",")  // All remaining emails in "CC"

    Try {
      val process = new ProcessBuilder("bash", shellScriptPath, alertCode, message, toEmail, ccEmails, business).start()

      val reader = new BufferedReader(new InputStreamReader(process.getInputStream))
      var line: String = null
      while ({ line = reader.readLine(); line != null }) {
        println(line)
      }

      val exitCode = process.waitFor()
      if (exitCode != 0) {
        throw new Exception(s"Error executing shell script: $shellScriptPath with exit code: $exitCode")
      } else {
        println(s"Email notification sent to: $toEmail and CC: $ccEmails for alertCode: $alertCode with message: $message")
      }
    } match {
      case Success(_) => println(s"Notification sent for alertCode: $alertCode")
      case Failure(ex) => println(s"Failed to send email notification: ${ex.getMessage}")
    }
  } else {
    println("No email addresses found to send the notification.")
  }
}

def processRecords(df: DataFrame): Unit = {
  df.foreachPartition { partition =>
    partition.foreach { row =>
      val alertCode = row.getAs[String]("alert_code")
      val dtCount = row.getAs[Int]("dt_count")
      val dateToLoad = row.getAs[String]("date_to_load")
      val bteqLocation = row.getAs[String]("bteq_location")
      val emailAddress = row.getAs[String]("email_address")
      val business = row.getAs[String]("business")

      val sourceTableName = row.getAs[String]("source_table_name").getOrElse {
        sendEmailNotification(alertCode, "Missing source_table_name", emailAddress, business)
        throw new Exception("Missing source_table_name")
      }

      val frequency = row.getAs[String]("frequency").getOrElse {
        sendEmailNotification(alertCode, "Missing frequency", emailAddress, business)
        throw new Exception("Missing frequency")
      }

      val filterColumn = row.getAs[String]("filter_column").getOrElse {
        sendEmailNotification(alertCode, "Missing filter_column", emailAddress, business)
        throw new Exception("Missing filter_column")
      }

      try {
        if (dtCount > 0) {
          val jdbcQuery = s"(SELECT COUNT(*) AS cnt FROM $sourceTableName WHERE $filterColumn = '$dateToLoad') AS subquery"
          
          val sourceTableCountDF = spark.read
            .format("jdbc")
            .option("url", jdbcUrl)
            .option("dbtable", jdbcQuery)
            .option("user", jdbcUser)
            .option("password", jdbcPassword)
            .option("driver", jdbcDriver)
            .load()

          val sourceTableCount = sourceTableCountDF.collect()(0).getAs[Long]("cnt")

          if (sourceTableCount == dtCount) {
            val sqlFolderPath = getSqlFolderPath(frequency, bteqLocation)
            
            if (Files.exists(Paths.get(sqlFolderPath))) {
              val sqlFilePath = s"$sqlFolderPath/${alertCode}.sql"
              
              if (Files.exists(Paths.get(sqlFilePath))) {
                runSqlScript(sqlFilePath)
                sendEmailNotification(alertCode, "SQL script executed successfully", emailAddress, business)
              } else {
                val message = s"SQL file not found for alertCode: $alertCode"
                sendEmailNotification(alertCode, message, emailAddress, business)
                println(message)
              }
            } else {
              val message = s"Folder not found for frequency: $frequency at path: $sqlFolderPath"
              sendEmailNotification(alertCode, message, emailAddress, business)
              println(message)
            }
          } else {
            val message = s"Source table count does not match DT_COUNT"
            sendEmailNotification(alertCode, message, emailAddress, business)
            println(message)
          }
import java.io.FileWriter
import java.sql.{Connection, DriverManager, ResultSet}

// JDBC connection properties
val connectionProperties = new java.util.Properties()
connectionProperties.put("user", jdbcUser)
connectionProperties.put("password", jdbcPassword)

// Function to export ResultSet to a CSV file
def writeResultSetToCSV(resultSet: ResultSet, outputPath: String): Unit = {
  val metaData = resultSet.getMetaData
  val columnCount = metaData.getColumnCount

  val fileWriter = new FileWriter(outputPath)
  try {
    // Write header
    val header = (1 to columnCount).map(metaData.getColumnName).mkString(",")
    fileWriter.write(header + "\n")

    // Write rows
    while (resultSet.next()) {
      val row = (1 to columnCount).map(resultSet.getString).mkString(",")
      fileWriter.write(row + "\n")
    }
    println(s"File successfully written to: $outputPath")
  } finally {
    fileWriter.close()
  }
}

// Get JDBC connection
val connection: Connection = DriverManager.getConnection(jdbcUrl, jdbcUser, jdbcPassword)

try {
  // Blank validation query
  val blankValidationQuery = 
    s"SELECT COUNT(*) AS cnt FROM U_D_DSV_001_RSS_0.RAMP_MASTER_TARGET2 WHERE variable = ''"
  val blankResult = connection.createStatement().executeQuery(blankValidationQuery)
  
  if (blankResult.next() && blankResult.getInt("cnt") > 0) {
    println("Blank values found. Updating blank values to NULL.")
    
    // Update query to set blank variables to NULL
    val updateQuery = 
      "UPDATE U_D_DSV_001_RSS_0.RAMP_MASTER_TARGET2 SET variable = NULL WHERE variable = ''"
    connection.createStatement().executeUpdate(updateQuery)
    println("Blank values updated to NULL.")
  }

  // Generate Master File 1
  val masterFile1Query = "SELECT * FROM U_D_DSV_001_RSS_0.RAMP_MASTER_TARGET1 ORDER BY 1"
  val masterFile1Result = connection.createStatement().executeQuery(masterFile1Query)
  val masterFile1Path = "/path/to/output/YYYYMMDDHHMMSS_Master_Table1.csv"
  writeResultSetToCSV(masterFile1Result, masterFile1Path)

  // Generate Master File 2
  val masterFile2Query = "SELECT * FROM U_D_DSV_001_RSS_0.RAMP_MASTER_TARGET2 ORDER BY 1,2"
  val masterFile2Result = connection.createStatement().executeQuery(masterFile2Query)
  val masterFile2Path = "/path/to/output/YYYYMMDDHHMMSS_Master_Table2.csv"
  writeResultSetToCSV(masterFile2Result, masterFile2Path)

  println("Master files successfully generated and saved.")

} catch {
  case ex: Exception =>
    println(s"Error during processing: ${ex.getMessage}")
    ex.printStackTrace()
} finally {
  connection.close()
}

        } else {
          val message = s"DT_COUNT is less than or equal to 0 for alertCode: $alertCode"
          sendEmailNotification(alertCode, message, emailAddress, business)
          throw new Exception(message)
        }
      } catch {
        case ex: Exception =>
          val message = s"Error processing alertCode: $alertCode - ${ex.getMessage}"
          sendEmailNotification(alertCode, message, emailAddress, business)
          println(message)
          ex.printStackTrace()
      }
    }
  }
}

def main(args: Array[String]): Unit = {
  val data = Seq(
    ("A001", 10, "2024-12-16", "/path/to/sql", "source_table", "daily", "filter_column", "business1", "email1@example.com"),
    ("A002", 5, "2024-12-17", "/path/to/sql", "source_table", "weekly", "filter_column", "business2", "email2@example.com")
  )

  import spark.implicits._
  val df = data.toDF("alert_code", "dt_count", "date_to_load", "bteq_location", "source_table_name", "frequency", "filter_column", "business", "email_address")

  processRecords(df)

  spark.stop
