import org.apache.spark.sql.{SparkSession, DataFrame}
import java.io.{BufferedReader, InputStreamReader}
import java.nio.file.{Files, Paths}
import java.text.SimpleDateFormat
import java.util.Calendar
import scala.util.{Try, Failure, Success}
import java.sql.{Connection, DriverManager, SQLException}
import scala.io.Source
import scala.util.Using

object EmailNotificationApp {

  // Define Spark session (initialize the Spark context)
  val spark: SparkSession = SparkSession.builder()
    .appName("EmailNotificationApp")
    .master("local[*]") // Adjust as needed for your environment
    .getOrCreate()

  // JDBC connection parameters (to be set with actual values)
  val jdbcUrl: String = sys.env.getOrElse("JDBC_URL", "jdbc:your_database_url")
  val jdbcUser: String = sys.env.getOrElse("JDBC_USER", "your_user")
  val jdbcPassword: String = sys.env.getOrElse("JDBC_PASSWORD", "your_password")
  val jdbcDriver: String = sys.env.getOrElse("JDBC_DRIVER", "com.jdbc.Driver") // Specify the correct JDBC driver class

  // Function to send email notifications via a shell script
  def sendEmailNotification(alertCode: String, message: String): Unit = {
    val shellScriptPath = "/path/to/email_notification.sh"  // Update with actual path to script
    Try {
      val process = new ProcessBuilder("bash", shellScriptPath, alertCode, message).start()

      // Capture the output of the shell script
      val reader = new BufferedReader(new InputStreamReader(process.getInputStream))
      var line: String = null
      while ({ line = reader.readLine(); line != null }) {
        println(line)  // You can log this output if necessary
      }

      // Check for process exit code
      val exitCode = process.waitFor()
      if (exitCode != 0) {
        throw new Exception(s"Error executing shell script: $shellScriptPath with exit code: $exitCode")
      } else {
        println(s"Email notification sent for alertCode: $alertCode with message: $message")
      }
    } match {
      case Success(_) => println(s"Notification sent for alertCode: $alertCode")
      case Failure(ex) => println(s"Failed to send email notification: ${ex.getMessage}")
    }
  }

  // Function to run SQL script
  def runSqlScript(scriptPath: String): Unit = {
    Try {
      // Check if the SQL file exists
      if (!Files.exists(Paths.get(scriptPath))) {
        throw new Exception(s"SQL file not found: $scriptPath")
      }

      // Read the content of the SQL file and drop the first 4 lines
      val sqlLines = Source.fromFile(scriptPath).getLines().drop(4).mkString("\n")

      if (sqlLines.isEmpty) {
        throw new Exception(s"SQL file at $scriptPath is empty after removing the first 4 lines.")
      }

      // JDBC Execution
      Using(DriverManager.getConnection(jdbcUrl, jdbcUser, jdbcPassword)) { conn =>
        val stmt = conn.createStatement()
        stmt.execute(sqlLines)
        println(s"SQL script executed successfully for: $scriptPath")
      } match {
        case Success(_) => println(s"SQL script executed successfully: $scriptPath")
        case Failure(ex) => println(s"Error running SQL script at $scriptPath: ${ex.getMessage}")
      }
    } match {
      case Success(_) => println(s"SQL script executed successfully: $scriptPath")
      case Failure(ex) =>
        println(s"Error running SQL script at $scriptPath: ${ex.getMessage}")
    }
  }

  // Function to get the folder path based on the frequency and day
  def getSqlFolderPath(frequency: String, bteqLocation: String): String = {
    frequency.toLowerCase match {
      case "daily" => 
        s"$bteqLocation/DAILY RUN" // Files arriving daily go to "DAILY RUN" folder

      case "weekly" =>
        // For weekly files, check which day of the week it is and route accordingly
        val dayOfWeek = new SimpleDateFormat("EEEE").format(Calendar.getInstance().getTime).toUpperCase
        dayOfWeek match {
          case "MONDAY" => s"$bteqLocation/MON"
          case "TUESDAY" => s"$bteqLocation/TUE"
          case "WEDNESDAY" => s"$bteqLocation/WED"
          case "THURSDAY" => s"$bteqLocation/THU"
          case "FRIDAY" => s"$bteqLocation/FRI"
          case "SATURDAY" => s"$bteqLocation/SAT"
          case "SUNDAY" => s"$bteqLocation/SUN"
          case _ => throw new Exception(s"Unknown weekday: $dayOfWeek")
        }

      case "biweekly" | "monthly" =>
        // Files arriving monthly or biweekly go to the "MONTHLY" folder
        s"$bteqLocation/MONTHLY"

      case _ => 
        throw new Exception(s"Unknown frequency: $frequency")
    }
  }

  // Main logic to process the DataFrame and send email notifications
  def processRecords(df: DataFrame): Unit = {
    df.foreachPartition { partition =>
      partition.foreach { row =>
        val alertCode = row.getAs[String]("alert_code")
        val dtCount = row.getAs[Int]("dt_count")
        val dateToLoad = row.getAs[String]("date_to_load")
        val bteqLocation = row.getAs[String]("bteq_location")

        val sourceTableName = row.getAs[String]("source_table_name").getOrElse {
          sendEmailNotification(alertCode, "Missing source_table_name")
          throw new Exception("Missing source_table_name")
        }

        val frequency = row.getAs[String]("frequency").getOrElse {
          sendEmailNotification(alertCode, "Missing frequency")
          throw new Exception("Missing frequency")
        }

        val filterColumn = row.getAs[String]("filter_column").getOrElse {
          sendEmailNotification(alertCode, "Missing filter_column")
          throw new Exception("Missing filter_column")
        }

        try {
          if (dtCount > 0) {
            // Run the query to count rows in the source table with the filter
            val jdbcQuery = s"(SELECT COUNT(*) AS cnt FROM $sourceTableName WHERE $filterColumn = '$dateToLoad') AS subquery"
            val sourceTableCountDF = spark.read
              .format("jdbc")
              .option("url", jdbcUrl)
              .option("dbtable", jdbcQuery)
              .option("user", jdbcUser)
              .option("password", jdbcPassword)
              .option("driver", jdbcDriver)
              .load()

            // Extract the count
            val sourceTableCount = sourceTableCountDF.collect()(0).getAs[Long]("cnt")

            // Compare counts
            if (sourceTableCount == dtCount) {
              val sqlFolderPath = getSqlFolderPath(frequency, bteqLocation)
              // Check if the folder exists
              if (Files.exists(Paths.get(sqlFolderPath))) {
                val sqlFilePath = s"$sqlFolderPath/${alertCode}.sql"
                if (Files.exists(Paths.get(sqlFilePath))) {
                  runSqlScript(sqlFilePath)
                  sendEmailNotification(alertCode, s"SQL script executed successfully for alertCode: $alertCode")
                } else {
                  val message = s"SQL file not found for alertCode: $alertCode in $sqlFolderPath"
                  sendEmailNotification(alertCode, message)
                  println(message)
                }
              } else {
                val message = s"Folder not found for frequency: $frequency at path: $sqlFolderPath"
                sendEmailNotification(alertCode, message)
                println(message)
              }
            } else {
              val message = s"Source table count ($sourceTableCount) does not match DT_COUNT ($dtCount) for alertCode: $alertCode"
              sendEmailNotification(alertCode, message)
              println(message)
            }
          } else {
            val message = s"DT_COUNT is less than or equal to 0 for alertCode: $alertCode"
            sendEmailNotification(alertCode, message)
            throw new Exception(message)
          }
        } catch {
          case ex: Exception =>
            val message = s"Error processing alertCode: $alertCode - ${ex.getMessage}"
            sendEmailNotification(alertCode, message)
            println(message)
            ex.printStackTrace()
        }
      }
    }
  }

  // Entry point for the Spark job
  def main(args: Array[String]): Unit = {
    // Simulate a DataFrame as an example (replace with actual DataFrame loading)
    val data = Seq(
      ("A001", 10, "2024-12-16", "/path/to/sql", Some("source_table"), Some("daily"), Some("filter_column")),
      ("A002", 5, "2024-12-17", "/path/to/sql", Some("source_table"), Some("weekly"), Some("filter_column"))
    )

    import spark.implicits._
    val df = data.toDF("alert_code", "dt_count", "date_to_load", "bteq_location", "source_table_name", "frequency", "filter_column")

    // Process the records and send notifications
    processRecords(df)

    // Stop the Spark session
    spark.stop()
  }
}
