import org.apache.spark.sql.SparkSession
import java.sql.{Connection, DriverManager, Statement}
import java.nio.file.{Files, Paths}
import java.nio.charset.StandardCharsets

object SqlUtils {

  // Fetch columns for a table in Spark SQL
  def getSparkTableColumns(jdbcUrl: String, jdbcUser: String, jdbcPassword: String, tableName: String): Seq[String] = {
    // Establish connection
    val connection = DriverManager.getConnection(jdbcUrl, jdbcUser, jdbcPassword)
    val query = 
      s"""
         |SELECT column_name 
         |FROM information_schema.columns 
         |WHERE table_name = '${tableName.split('.').last}' AND table_schema = '${tableName.split('.').head}'
         |ORDER BY ordinal_position
       """.stripMargin
    val statement = connection.createStatement()
    
    try {
      val resultSet = statement.executeQuery(query)
      val columns = Iterator.continually(resultSet).takeWhile(_.next()).map(_.getString("column_name")).toSeq
      if (columns.isEmpty) throw new Exception(s"No columns found for table: $tableName")
      columns
    } finally {
      statement.close()
      connection.close()
    }
  }

  // Process insert queries with missing column names
  def processInsertQueries(jdbcUrl: String, jdbcUser: String, jdbcPassword: String, jdbcDriver: String, tableName: String, query: String): String = {
    val columns = getSparkTableColumns(jdbcUrl, jdbcUser, jdbcPassword, tableName)
    val columnsString = columns.mkString(", ")
    query.replace(s"INSERT INTO $tableName", s"INSERT INTO $tableName ($columnsString)")
  }
}

object SqlScriptExecutor {
  
  def runSqlScript(scriptPath: String, jdbcUrl: String, jdbcUser: String, jdbcPassword: String, jdbcDriver: String, tableNames: Seq[String]): Unit = {
    println(s"Running SQL script from: $scriptPath")
    val logPath = s"sql_execution_log_${System.currentTimeMillis()}.txt"
    val logBuffer = new StringBuilder

    // Register JDBC driver
    Class.forName(jdbcDriver)

    // Establish connection
    val connection = DriverManager.getConnection(jdbcUrl, jdbcUser, jdbcPassword)
    val statement = connection.createStatement()

    // Read SQL script
    val script = Files.readString(Paths.get(scriptPath), StandardCharsets.UTF_8)

    // Split script into commands and execute them
    val commands = script.split(";").map(_.trim).filter(_.nonEmpty)

    tableNames.foreach { tableName =>
      commands.foreach { command =>
        try {
          val updatedCommand = 
            if (command.toLowerCase.startsWith(s"insert into $tableName".toLowerCase))
              SqlUtils.processInsertQueries(jdbcUrl, jdbcUser, jdbcPassword, jdbcDriver, tableName, command)
            else command

          println(s"Executing command: $updatedCommand")
          statement.execute(updatedCommand)
          logBuffer.append(s"[SUCCESS] Executed: $updatedCommand\n")
        } catch {
          case ex: Exception =>
            val errorMsg = s"[ERROR] Failed to execute: $command\nMessage: ${ex.getMessage}"
            println(errorMsg)
            logBuffer.append(errorMsg + "\n")
        }
      }
    }

    // Save log to file
    Files.write(Paths.get(logPath), logBuffer.toString().getBytes(StandardCharsets.UTF_8))
    println(s"Execution log saved to: $logPath")

    // Close resources
    statement.close()
    connection.close()
  }

  def main(args: Array[String]): Unit = {
    val scriptPath = "path/to/your/sqlfile.sql"
    val jdbcUrl = "jdbc:spark://your-spark-cluster-url"
    val jdbcUser = "your_spark_user"
    val jdbcPassword = "your_spark_password"
    val jdbcDriver = "org.apache.hive.jdbc.HiveDriver"

    // List of tables
    val tableNames = Seq("database_name.table_name1", "database_name.table_name2")

    runSqlScript(scriptPath, jdbcUrl, jdbcUser, jdbcPassword, jdbcDriver, tableNames)
  }
}
