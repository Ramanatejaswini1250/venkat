def resultSetToDataFrame(resultSet: ResultSet, spark: SparkSession): DataFrame = {
  import spark.implicits._

  val metadata = resultSet.getMetaData
  val columnCount = metadata.getColumnCount
  val columnNames = (1 to columnCount).map(metadata.getColumnName)

  val rows = mutable.Buffer[Row]()

  // Ensure that resultSet has data before processing
  var hasData = false
  while (resultSet.next()) {  // Moves the cursor to the first row and processes each row
    hasData = true
    val row = (1 to columnCount).map { i =>
      Option(resultSet.getObject(i)).orNull  // Null-safe retrieval of values
    }.toSeq
    rows.append(Row.fromSeq(row))
  }

  // If no data was found, return an empty DataFrame instead of throwing an error
  if (!hasData) {
    println("Warning: No data found in ResultSet. Returning an empty DataFrame.")
    return spark.createDataFrame(spark.emptyDataFrame.rdd, StructType(Seq.empty))
  }

  // Define schema dynamically based on column types
  val schema = StructType(columnNames.map { name =>
    val columnType = metadata.getColumnTypeName(columnNames.indexOf(name) + 1)
    val fieldType = columnType match {
      case "VARCHAR" | "CHAR" | "TEXT" => StringType
      case "INTEGER" | "INT" => IntegerType
      case "BIGINT" => LongType
      case "DECIMAL" | "NUMERIC" => DecimalType(10, 2)
      case "DATE" | "TIMESTAMP" => TimestampType
      case "DOUBLE" | "FLOAT" => DoubleType
      case "BOOLEAN" => BooleanType
      case _ => StringType // Default to StringType for unknown types
    }
    StructField(name, fieldType, nullable = true)
  })

  spark.createDataFrame(rows.toSeq.asJava, schema)
}
