import java.sql.Types._
import org.apache.spark.sql.types._

def resultSetToDataFrame(resultSet: ResultSet, spark: SparkSession): DataFrame = {
  import spark.implicits._

  val metadata = resultSet.getMetaData
  val columnCount = metadata.getColumnCount

  if (columnCount <= 0) {
    throw new RuntimeException(s"Invalid column count: $columnCount")
  }

  // Detect column names and types
  val columnNames = (1 to columnCount).map(metadata.getColumnName)
  val columnTypes = (1 to columnCount).map(metadata.getColumnType)

  val rows = mutable.Buffer[Row]()

  while (resultSet.next()) {
    val row = (1 to columnCount).map { i =>
      // Depending on the column type, use appropriate get methods
      columnTypes(i - 1) match {
        case INTEGER     => Option(resultSet.getInt(i)).getOrElse(null.asInstanceOf[Any])  // Integer
        case VARCHAR     => Option(resultSet.getString(i)).getOrElse(null.asInstanceOf[Any]) // String
        case DATE        => Option(resultSet.getDate(i)).getOrElse(null.asInstanceOf[Any])   // Date
        case TIMESTAMP   => Option(resultSet.getTimestamp(i)).getOrElse(null.asInstanceOf[Any]) // Timestamp
        case FLOAT       => Option(resultSet.getFloat(i)).getOrElse(null.asInstanceOf[Any])  // Float
        case DOUBLE      => Option(resultSet.getDouble(i)).getOrElse(null.asInstanceOf[Any]) // Double
        case BIGINT      => Option(resultSet.getLong(i)).getOrElse(null.asInstanceOf[Any])   // BigInt
        case BOOLEAN     => Option(resultSet.getBoolean(i)).getOrElse(null.asInstanceOf[Any]) // Boolean
        case DECIMAL     => Option(resultSet.getBigDecimal(i)).getOrElse(null.asInstanceOf[Any]) // Decimal
        case LONGVARCHAR => Option(resultSet.getString(i)).getOrElse(null.asInstanceOf[Any]) // Large String (e.g., CLOB)
        case _           => Option(resultSet.getObject(i)).getOrElse(null.asInstanceOf[Any]) // Fallback to generic Object
      }
    }.toSeq
    rows.append(Row.fromSeq(row))
  }

  if (rows.isEmpty) {
    throw new RuntimeException("No data found in ResultSet.")
  }

  // Create the schema dynamically based on column types
  val schema = StructType(columnNames.zip(columnTypes).map {
    case (name, sqlType) =>
      val sparkType = sqlType match {
        case INTEGER      => IntegerType
        case VARCHAR      => StringType
        case DATE         => DateType
        case TIMESTAMP    => TimestampType
        case FLOAT        => FloatType
        case DOUBLE       => DoubleType
        case BIGINT       => LongType
        case BOOLEAN      => BooleanType
        case DECIMAL      => DecimalType(10, 2) // Adjust scale and precision as needed
        case LONGVARCHAR  => StringType // For CLOB or large strings
        case _            => StringType // Default fallback to StringType
      }
      StructField(name, sparkType, nullable = true)
  })

  // Return the DataFrame with the correct schema
  spark.createDataFrame(rows.toSeq.asJava, schema)
}
