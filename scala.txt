import org.apache.spark.sql.{Row, SparkSession, DataFrame}
import org.apache.spark.sql.types._
import org.apache.spark.broadcast.Broadcast

// Create Spark Session
val spark = SparkSession.builder()
  .appName("Broadcast Example")
  .getOrCreate()

// JDBC Connection Details
val jdbcUrl = "jdbc:mysql://your-db-host:3306/your-database"
val dbTable = "master_table"
val dbUser = "your-username"
val dbPassword = "your-password"

// Step 1: Load master data before foreachPartition
val masterDF = spark.read.format("jdbc")
  .option("url", jdbcUrl)
  .option("dbtable", dbTable)
  .option("user", dbUser)
  .option("password", dbPassword)
  .load()

// Step 2: Collect master data and broadcast it
val masterData: Array[Row] = masterDF.collect() // Collects data from DataFrame as an Array of Rows
val broadcastMaster: Broadcast[Array[Row]] = spark.sparkContext.broadcast(masterData)

// Step 3: Define schema (for reconstructing DataFrame inside foreachPartition)
val masterSchema: StructType = masterDF.schema 

// Example DataFrame to process
val df = spark.read.parquet("/path/to/your/data.parquet")

// Step 4: Use broadcasted data inside foreachPartition with conditions
df.foreachPartition(partition => {
  // Get broadcasted master data inside each partition
  val masterList = broadcastMaster.value 

  // Convert to RDD and recreate DataFrame
  val masterRDD = spark.sparkContext.parallelize(masterList)
  val masterDF = spark.createDataFrame(masterRDD, masterSchema)

  partition.foreach(row => {
    // Read values from the row
    val sourceCount = row.getAs[Int]("source_count") // Replace with correct column name
    val dtCount = row.getAs[Int]("dt_count")         // Replace with correct column name
    val masterCount = masterDF.count().toInt         // Count rows in masterDF

    // Apply your condition
    if (sourceCount == dtCount) {
      if (masterCount == dtCount) {
        println("Condition met, using masterDF")
        masterDF.show()  // Perform required operation on masterDF
      }
    }
  })
})

