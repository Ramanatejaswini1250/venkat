df.foreachPartition(partition => {
  // Step 1: Open DB connection per partition
  val conn = DriverManager.getConnection(jdbcUrl, dbUser, dbPassword)
  val stmt = conn.createStatement()

  try {
    partition.foreach(row => {
      val sourceCount = row.getAs[Int]("source_count")
      val dtCount = row.getAs[Int]("dt_count")

      if (sourceCount == dtCount) {
        println("Executing SQL script inside partition...")

        val isSqlSuccess = runSqlScript() // Run SQL inside partition

        if (isSqlSuccess) {
          println("SQL execution successful. Fetching master table data...")

          var masterDF: DataFrame = null
          var maxWaitTime = 300
          var elapsedTime = 0
          val checkInterval = 10

          while (elapsedTime < maxWaitTime) {
            try {
              // Step 2: Execute query using stmt
              val resultSet = stmt.executeQuery("SELECT * FROM master_table")

              // Step 3: Convert ResultSet to DataFrame
              val masterList = new ListBuffer[Row]()
              val schema = StructType(Seq(
                StructField("col1", StringType, true),
                StructField("col2", IntegerType, true) // Define schema as per table
              ))

              while (resultSet.next()) {
                val row = Row(resultSet.getString(1), resultSet.getInt(2))
                masterList += row
              }

              resultSet.close()

              if (masterList.nonEmpty) {
                println(s"Data found in master_table after $elapsedTime seconds.")
                masterDF = spark.createDataFrame(spark.sparkContext.parallelize(masterList), schema)
                break // Exit loop
              }
            } catch {
              case e: Exception =>
                println(s"DB Read Error: ${e.getMessage}")
            }

            println(s"No data found. Retrying in $checkInterval seconds...")
            Thread.sleep(checkInterval * 1000)
            elapsedTime += checkInterval
          }

          if (masterDF == null || masterDF.count() == 0) {
            println("Timeout reached or no data available. Skipping this partition.")
          } else {
            // Step 4: Broadcast master data inside partition
            val masterData: Array[Row] = masterDF.collect()
            val broadcastMaster = spark.sparkContext.broadcast(masterData)

            // Step 5: Use broadcast data
            val masterRDD = spark.sparkContext.parallelize(broadcastMaster.value)
            val masterDF = spark.createDataFrame(masterRDD, masterDF.schema)

            val masterCount = masterDF.count().toInt

            if (masterCount == dtCount) {
              println("Condition met, using masterDF")
              masterDF.show()
            }
          }
        } else {
          println("SQL script execution failed. Skipping this partition.")
        }
      }
    })
  } finally {
    // Step 6: Close DB connection after processing the partition
    stmt.close()
    conn.close()
  }
})

