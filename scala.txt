import org.apache.spark.sql.{Row, SparkSession}
import org.apache.spark.sql.types._
import java.sql.{Connection, DriverManager, ResultSet, Statement}
import scala.collection.mutable.ListBuffer

// Initialize SparkSession
val spark: SparkSession = SparkSession.builder()
  .appName("Broadcast Master Data")
  .getOrCreate()

// Define JDBC credentials
val jdbcUrl = "jdbc:mysql://your-db-host:3306/your-db"
val dbUser = "your-username"
val dbPassword = "your-password"

// Define master table schema
val masterSchema = StructType(Seq(
  StructField("col1", StringType, nullable = true),
  StructField("col2", IntegerType, nullable = true)
))

df.foreachPartition(partition => {
  // Step 1: Open DB connection per partition
  val conn: Connection = DriverManager.getConnection(jdbcUrl, dbUser, dbPassword)
  val stmt: Statement = conn.createStatement()

  try {
    partition.foreach(row => {
      val sourceCount = row.getAs[Int]("source_count")
      val dtCount = row.getAs[Int]("dt_count")

      if (sourceCount == dtCount) {
        println("Executing SQL script inside partition...")

        val isSqlSuccess = runSqlScript() // Execute SQL to load data

        if (isSqlSuccess) {
          println("SQL execution successful. Fetching master table data...")

          var masterList = new ListBuffer[Row]()

          try {
            // Step 2: Execute query using stmt
            val resultSet: ResultSet = stmt.executeQuery("SELECT * FROM master_table")

            // Step 3: Collect data from ResultSet into List[Row]
            while (resultSet.next()) {
              val row = Row(resultSet.getString("col1"), resultSet.getInt("col2"))
              masterList += row
            }
            resultSet.close()
          } catch {
            case e: Exception =>
              println(s"DB Read Error: ${e.getMessage}")
          }

          // Step 4: Broadcast the collected data (not DataFrame)
          if (masterList.nonEmpty) {
            println("Broadcasting master data...")
            val broadcastMaster = spark.sparkContext.broadcast(masterList.toArray)

            // Step 5: Recreate the DataFrame inside the partition using schema
            val masterRDD = spark.sparkContext.parallelize(broadcastMaster.value)
            val masterDF = spark.createDataFrame(masterRDD, masterSchema)

            val masterCount = masterDF.count().toInt

            if (masterCount == dtCount) {
              println("Condition met, using masterDF")
              masterDF.show()
            }
          } else {
            println("No data found in master_table. Skipping this partition.")
          }
        } else {
          println("SQL script execution failed. Skipping this partition.")
        }
      }
    })
  } finally {
    // Step 6: Close DB connection after processing the partition
    stmt.close()
    conn.close()
  }
})

