import org.apache.hadoop.fs.{FileSystem, Path}
import org.apache.hadoop.conf.Configuration
import org.apache.spark.sql.SparkSession
import java.util.Timer
import java.util.TimerTask

object HDFSFileCleaner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("HDFS File Cleaner")
      .getOrCreate()

    val conf = new Configuration()
    val fs = FileSystem.get(conf)
    val path = new Path("/tmp/your-hdfs-dir/")

    // Timer to schedule deletion every 2 hours
    val timer = new Timer()

    val task = new TimerTask() {
      override def run(): Unit = {
        // Delete the files in the HDFS directory
        val isDeleted = fs.delete(path, true)
        if (isDeleted) {
          println(s"Successfully deleted files at ${System.currentTimeMillis()}")
        } else {
          println(s"Failed to delete files at ${System.currentTimeMillis()}")
        }
      }
    }

    // Schedule task every 2 hours (7200000 ms)
    timer.schedule(task, 0, 7200000)

    // Keep the Spark job running to allow the timer to keep running
    spark.streams.awaitAnyTermination()
  }
}
