import org.apache.spark.sql.{DataFrame, Row, SparkSession}
import org.apache.spark.sql.types._
import java.sql.{Connection, DriverManager, ResultSet}
import java.io.File
import org.apache.commons.io.FileUtils

// Initialize Spark Session
val spark = SparkSession.builder()
  .appName("Master Table Export and Validation")
  .config("spark.master", "local[*]")
  .getOrCreate()

// Database connection properties
val jdbcUrl = "jdbc:mysql://your-database-url:3306/your-database"
val jdbcUser = "your-username"
val jdbcPassword = "your-password"
val jdbcDriver = "com.mysql.cj.jdbc.Driver"

// Define the query for the Master Table
val masterTable1Query =
  """
    SELECT * 
    FROM U_D_DSV_001_RSS_0.RAMP_MASTER_TARGET1_TEST 
    ORDER BY 1
  """

// Generate the file path with a timestamp
val currentTimestamp = java.time.LocalDateTime.now().toString.replace(":", "").replace(".", "")
val finalFolderPath = "/path/to/your/output/directory" // Replace with your folder path
val masterFile1Path = s"$finalFolderPath/$currentTimestamp_Master_Table1.csv"

// Set up a JDBC connection
Class.forName(jdbcDriver)
val connection: Connection = DriverManager.getConnection(jdbcUrl, jdbcUser, jdbcPassword)
val statement = connection.createStatement()

// Execute the query to fetch data
val masterTable1: ResultSet = statement.executeQuery(masterTable1Query)

// Define the schema for the Master Table
val schema_master1 = StructType(
  Seq(
    StructField("ALERT_ID", StringType, nullable = true),
    StructField("ALERT_CODE", StringType, nullable = true),
    StructField("BUSINESS_LINE", StringType, nullable = true),
    StructField("EVENT_TIMESTAMP", StringType, nullable = true),
    StructField("PRIORITY", IntegerType, nullable = true),
    StructField("ALERT_DUE_DATE", StringType, nullable = true)
  )
)

// Convert the ResultSet to Rows
val rows_master1 = Iterator
  .continually((masterTable1.next(), masterTable1))
  .takeWhile(_._1)
  .map { case (_, rs) =>
    Row(
      rs.getString("ALERT_ID"),
      rs.getString("ALERT_CODE"),
      rs.getString("BUSINESS_LINE"),
      rs.getString("EVENT_TIMESTAMP"),
      rs.getInt("PRIORITY"),
      rs.getString("ALERT_DUE_DATE")
    )
  }
  .toList

// Create a DataFrame from the Rows
val masterTable1DF = spark.createDataFrame(
  spark.sparkContext.parallelize(rows_master1),
  schema_master1
)

// Write the DataFrame to a temporary directory with coalesce
val tempOutputPath = s"$finalFolderPath/tmp"
masterTable1DF
  .coalesce(1) // Ensure a single part file
  .write
  .option("header", "true") // Include column headers
  .option("delimiter", ",") // Use comma as delimiter
  .mode("overwrite") // Overwrite if file already exists
  .csv(tempOutputPath)

// Move the single CSV part file to the final destination
val tmpDir = new File(tempOutputPath)
val finalCsv = new File(masterFile1Path)

// Find and move the part file to the desired location
val partFile = tmpDir.listFiles().find(_.getName.startsWith("part")).get
FileUtils.moveFile(partFile, finalCsv)
FileUtils.deleteDirectory(tmpDir) // Clean up the temporary directory

println(s"Data successfully exported to CSV: $masterFile1Path")

// Validate the output
val masterFile1CSVDF = spark.read
  .option("header", "true")
  .option("delimiter", ",")
  .csv(masterFile1Path)

// Function to validate two DataFrames
def validateData(df1: DataFrame, df2: DataFrame): Boolean = {
  df1.except(df2).isEmpty && df2.except(df1).isEmpty
}

println("Starting data validation...")
val isValid = validateData(masterTable1DF, masterFile1CSVDF)

if (isValid) {
  println("Data validation successful: Exported CSV matches Master Table.")
} else {
  println(
    s"""
       |Data validation failed!
       |Source row count: ${masterTable1DF.count()}
       |CSV row count: ${masterFile1CSVDF.count()}
       |Check for mismatches between the source table and the exported file.
       |""".stripMargin
  )
}

// Clean up JDBC connection
statement.close()
connection.close()

// Stop SparkSession
spark.stop()
