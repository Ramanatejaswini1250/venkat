import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

// Define schema for Master1
val schema_master1 = StructType(
  Seq(
    StructField("ALERT_ID", StringType, nullable = true),
    StructField("ALERT_CODE", StringType, nullable = true),
    StructField("BUSINESS_LINE", StringType, nullable = true),
    StructField("EVENT_TIMESTAMP", StringType, nullable = true),
    StructField("PRIORITY", IntegerType, nullable = true),
    StructField("ALERT_DUE_DATE", StringType, nullable = true)
  )
)

// Execute Query
val masterTable1Query = "SELECT ALERT_ID, ALERT_CODE, BUSINESS_LINE, EVENT_TIMESTAMP, PRIORITY, ALERT_DUE_DATE FROM your_table"
val statement = connection.createStatement()
val masterTable1: ResultSet = statement.executeQuery(masterTable1Query)

// Read ResultSet into List of Rows
import scala.collection.mutable.ListBuffer
val rows_master1 = new ListBuffer[Row]()

while (masterTable1.next()) {
  rows_master1 += Row(
    masterTable1.getString("ALERT_ID"),
    masterTable1.getString("ALERT_CODE"),
    masterTable1.getString("BUSINESS_LINE"),
    masterTable1.getString("EVENT_TIMESTAMP"),
    masterTable1.getInt("PRIORITY"),
    masterTable1.getString("ALERT_DUE_DATE")
  )
}

// Convert ListBuffer to DataFrame
val masterTable1DF = spark.createDataFrame(
  spark.sparkContext.parallelize(rows_master1.toList),
  schema_master1
)

// Show DataFrame
masterTable1DF.show()
)

// Convert and format the date columns (EVENT_TIMESTAMP and ALERT_DUE_DATE)
// The rest of the columns will remain unchanged
val formattedMasterTable1DF = masterTable1DF
  .withColumn("EVENT_TIMESTAMP", 
    date_format(to_timestamp(col("EVENT_TIMESTAMP"), "dd/MM/yyyy hh:mm:ss a"), "dd-MM-yyyy HH:mm"))
  .withColumn("ALERT_DUE_DATE", 
    date_format(to_timestamp(col("ALERT_DUE_DATE"), "dd/MM/yyyy hh:mm:ss a"), "dd-MM-yyyy HH:mm"))

// Now formattedMasterTable1DF contains all columns with the formatted date columns
// Write the formatted DataFrame to a CSV file
formattedMasterTable1DF
  .coalesce(1) // Ensure a single part file
  .write
  .option("header", "true") // Include column headers
  .option("delimiter", ",") // Use comma as delimiter
  .mode("overwrite") // Overwrite if file already exists
  .csv(tempOutputPath)

// Move the single CSV part file to the final destination
val tmpDir = new File(tempOutputPath)
val finalCsv = new File(masterFile1Path)
val partFile = tmpDir.listFiles().find(_.getName.startsWith("part")).get
FileUtils.moveFile(partFile, finalCsv)
FileUtils.deleteDirectory(tmpDir) // Clean up the temporary directory

println(s"Data successfully exported to CSV: $masterFile1Path")

// Data validation (same as before)
val masterFile1CSVDF = spark.read
  .option("header", "true")
  .option("delimiter", ",")
  .csv(masterFile1Path)

val isValid = validateData(formattedMasterTable1DF, masterFile1CSVDF)

if (isValid) {
  println("Data validation successful: Exported CSV matches Master Table.")
} else {
  println(
    s"""
       |Data validation failed!
       |Source row count: ${formattedMasterTable1DF.count()}
       |CSV row count: ${masterFile1CSVDF.count()}
       |Check for mismatches between the source table and the exported file.
       |""".stripMargin
  )
}

// Clean up JDBC connection (same as before)
statement.close()
connection.close()

// Stop SparkSession (same as before)
spark.stop()
