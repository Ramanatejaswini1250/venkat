import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

// Define the query for the Master Table (same as before)
val masterTable1Query =
  """
    SELECT * 
    FROM U_D_DSV_001_RSS_0.RAMP_MASTER_TARGET1_TEST 
    ORDER BY 1
  """

// Define the schema for the Master Table (same as before)
val schema_master1 = StructType(
  Seq(
    StructField("ALERT_ID", StringType, nullable = true),
    StructField("ALERT_CODE", StringType, nullable = true),
    StructField("BUSINESS_LINE", StringType, nullable = true),
    StructField("EVENT_TIMESTAMP", StringType, nullable = true),
    StructField("PRIORITY", IntegerType, nullable = true),
    StructField("ALERT_DUE_DATE", StringType, nullable = true)
  )
)

// Fetch the data from the database (same as before)
val masterTable1: ResultSet = statement.executeQuery(masterTable1Query)
val rows_master1 = Iterator
  .continually((masterTable1.next(), masterTable1))
  .takeWhile(_._1)
  .map { case (_, rs) =>
    Row(
      rs.getString("ALERT_ID"),
      rs.getString("ALERT_CODE"),
      rs.getString("BUSINESS_LINE"),
      rs.getString("EVENT_TIMESTAMP"),
      rs.getInt("PRIORITY"),
      rs.getString("ALERT_DUE_DATE")
    )
  }
  .toList

// Create a DataFrame from the Rows
val masterTable1DF = spark.createDataFrame(
  spark.sparkContext.parallelize(rows_master1),
  schema_master1
)

// Convert and format the date columns (EVENT_TIMESTAMP and ALERT_DUE_DATE)
// The rest of the columns will remain unchanged
val formattedMasterTable1DF = masterTable1DF
  .withColumn("EVENT_TIMESTAMP", 
    date_format(to_timestamp(col("EVENT_TIMESTAMP"), "dd/MM/yyyy hh:mm:ss a"), "dd-MM-yyyy HH:mm"))
  .withColumn("ALERT_DUE_DATE", 
    date_format(to_timestamp(col("ALERT_DUE_DATE"), "dd/MM/yyyy hh:mm:ss a"), "dd-MM-yyyy HH:mm"))

// Now formattedMasterTable1DF contains all columns with the formatted date columns
// Write the formatted DataFrame to a CSV file
formattedMasterTable1DF
  .coalesce(1) // Ensure a single part file
  .write
  .option("header", "true") // Include column headers
  .option("delimiter", ",") // Use comma as delimiter
  .mode("overwrite") // Overwrite if file already exists
  .csv(tempOutputPath)

// Move the single CSV part file to the final destination
val tmpDir = new File(tempOutputPath)
val finalCsv = new File(masterFile1Path)
val partFile = tmpDir.listFiles().find(_.getName.startsWith("part")).get
FileUtils.moveFile(partFile, finalCsv)
FileUtils.deleteDirectory(tmpDir) // Clean up the temporary directory

println(s"Data successfully exported to CSV: $masterFile1Path")

// Data validation (same as before)
val masterFile1CSVDF = spark.read
  .option("header", "true")
  .option("delimiter", ",")
  .csv(masterFile1Path)

val isValid = validateData(formattedMasterTable1DF, masterFile1CSVDF)

if (isValid) {
  println("Data validation successful: Exported CSV matches Master Table.")
} else {
  println(
    s"""
       |Data validation failed!
       |Source row count: ${formattedMasterTable1DF.count()}
       |CSV row count: ${masterFile1CSVDF.count()}
       |Check for mismatches between the source table and the exported file.
       |""".stripMargin
  )
}

// Clean up JDBC connection (same as before)
statement.close()
connection.close()

// Stop SparkSession (same as before)
spark.stop()
