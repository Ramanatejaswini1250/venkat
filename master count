import org.apache.spark.sql.{DataFrame, Row, SparkSession}
import org.apache.spark.sql.types._
import java.io.File
import org.apache.commons.io.FileUtils

object MasterTableToCSV {

  def main(args: Array[String]): Unit = {

    // Initialize SparkSession
    val spark = SparkSession.builder()
      .appName("MasterTableToCSV")
      .master("local[*]") // Use local mode for testing
      .getOrCreate()

    // JDBC connection details
    val jdbcUrl = "jdbc:mysql://your-database-url:3306/your-database"
    val jdbcUser = "your-username"
    val jdbcPassword = "your-password"
    val jdbcDriver = "com.mysql.cj.jdbc.Driver"

    // Table query
    val masterTable1Query = s"""
      SELECT * 
      FROM your_database.your_table_name
      ORDER BY 1
    """

    // File path for saving the CSV
    val finalFolderPath = "/path/to/output/directory"
    val currentTimestamp = new java.text.SimpleDateFormat("yyyyMMddHHmmss").format(new java.util.Date())
    val masterFile1Path = s"$finalFolderPath/${currentTimestamp}_Master_Table1.csv"

    // Define schema for the table (adjust as per your table structure)
    val schema_master1 = StructType(
      Seq(
        StructField("ALERT_ID", StringType, nullable = true),
        StructField("ALERT_CODE", StringType, nullable = true),
        StructField("BUSINESS_LINE", StringType, nullable = true),
        StructField("EVENT_TIMESTAMP", StringType, nullable = true),
        StructField("PRIORITY", IntegerType, nullable = true),
        StructField("ALERT_DUE_DATE", StringType, nullable = true)
      )
    )

    // Fetch data from the database into a DataFrame
    val masterTable1DF = spark.read
      .format("jdbc")
      .option("url", jdbcUrl)
      .option("dbtable", s"(${masterTable1Query}) as tmp")
      .option("user", jdbcUser)
      .option("password", jdbcPassword)
      .option("driver", jdbcDriver)
      .schema(schema_master1) // Optional: If schema is known
      .load()

    // Write DataFrame to a single CSV file
    masterTable1DF
      .coalesce(1) // Combine all partitions into one
      .write
      .option("header", "true") // Include column headers
      .option("delimiter", ",") // Specify the column delimiter
      .mode("overwrite") // Overwrite if the file already exists
      .csv(s"$finalFolderPath/tmp") // Write to a temporary directory

    // Move the single part file to the final desired location
    val tmpDir = new File(s"$finalFolderPath/tmp")
    val finalCsv = new File(masterFile1Path)

    // Locate the part file in the temporary directory
    val partFile = tmpDir.listFiles().find(_.getName.startsWith("part")).get

    // Rename (or move) the part file to the final CSV path
    FileUtils.moveFile(partFile, finalCsv)

    // Delete the temporary directory
    FileUtils.deleteDirectory(tmpDir)

    println(s"Master Table 1 data written to a single CSV file at: $masterFile1Path")

    // Stop SparkSession
    spark.stop()
  }
}
