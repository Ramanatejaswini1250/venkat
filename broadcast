import org.apache.spark.sql.{Row, SparkSession}
import org.apache.spark.sql.types._
import scala.collection.mutable.ArrayBuffer

// Create Spark session
val spark = SparkSession.builder()
  .appName("FixNullPointerException")
  .getOrCreate()

// Define schema based on screenshot data
val schema_master1 = StructType(Seq(
  StructField("alert_id", StringType, nullable = true),
  StructField("alert_code", StringType, nullable = true),
  StructField("business_line", StringType, nullable = true),
  StructField("event_timestamp", StringType, nullable = true),
  StructField("priority", IntegerType, nullable = true),  // Ensure safe handling
  StructField("alert_due_date", StringType, nullable = true)
))

// Collect rows
val rows_master1 = new ArrayBuffer[Row]()

if (masterTable1.next()) {  // Check for the first row
  do {
    val alertId = Option(masterTable1.getString("alert_id")).getOrElse(null)
    val alertCode = Option(masterTable1.getString("alert_code")).getOrElse(null)
    val businessLine = Option(masterTable1.getString("business_line")).getOrElse(null)
    val eventTimestamp = Option(masterTable1.getString("event_timestamp")).getOrElse(null)
    val alertDueDate = Option(masterTable1.getString("alert_due_date")).getOrElse(null)

    val priority = Option(masterTable1.getObject("priority")) match {
      case Some(value: Int) => value
      case _ => null
    }

    // Add the row to the buffer
    rows_master1 += Row(alertId, alertCode, businessLine, eventTimestamp, priority, alertDueDate)
  } while (masterTable1.next())  // Keep iterating over all rows
}


  // Create Row object
  val row = Row(alertId, alertCode, businessLine, eventTimestamp, priority, alertDueDate)
  rows_master1 += row
}

// Debugging: Print collected rows before DataFrame creation
println(s"Schema: $schema_master1")
println(s"Row Count: ${rows_master1.size}")
rows_master1.foreach(row => println(s"Row Data: $row"))

// Ensure rows are not empty before creating DataFrame
val masterTable1DF = if (rows_master1.isEmpty) {
  println("Warning: No data found. Creating an empty DataFrame.")
  spark.createDataFrame(spark.sparkContext.emptyRDD[Row], schema_master1)
} else {
  spark.createDataFrame(rows_master1.toSeq, schema_master1)  // Convert to immutable Seq
}

// Show DataFrame contents
masterTable1DF.show()
