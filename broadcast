import org.apache.spark.sql.functions.{col, lower, date_format}
import org.apache.spark.sql.{Row, SparkSession}
import org.apache.spark.sql.types.{StructType, StructField, StringType}
import java.sql.{Connection, DriverManager, ResultSet, Statement}

// Initialize Spark session
val spark = SparkSession.builder()
  .appName("RampAutomationExecution")
  .master("local[*]")  // Change master based on your cluster setup
  .getOrCreate()

val masterTable1Query =
  s"""
  |SELECT 
  |  ALERT_ID AS alert_id, 
  |  ALERT_CODE AS alert_code, 
  |  BUSINESS_LINE AS business_line, 
  |  EVENT_TIMESTAMP AS event_timestamp 
  |FROM U_D_DSV_001_RSS_0.RAMP_MASTER_TARGET1_TEST
  |WHERE alert_code = '$alertCode' 
  |ORDER BY 1
  """.stripMargin

// Execute query
val masterTable1: ResultSet = stmt_rss.executeQuery(masterTable1Query)

// Get column names dynamically
val metaData = masterTable1.getMetaData
val columnCount = metaData.getColumnCount
val availableColumns = (1 to columnCount).map(i => metaData.getColumnName(i).toLowerCase).toSet

println("Available Columns in ResultSet:")
availableColumns.foreach(println)  // Debugging: Prints detected column names

// Define schema dynamically based on detected columns
val schema_master1 = StructType(Seq(
  StructField("alert_id", StringType, nullable = true),
  StructField("alert_code", StringType, nullable = true),
  StructField("business_line", StringType, nullable = true),
  StructField("event_timestamp", StringType, nullable = true)
))

// Ensure there is data before processing
if (masterTable1.isBeforeFirst()) {
  val rows_master1 = Iterator
    .continually(masterTable1)
    .takeWhile(_.next())
    .map { rs =>
      Row(
        rs.getString(availableColumns.find(_.contains("alert_id")).getOrElse("ALERT_ID")),
        rs.getString(availableColumns.find(_.contains("alert_code")).getOrElse("ALERT_CODE")),
        rs.getString(availableColumns.find(_.contains("business_line")).getOrElse("BUSINESS_LINE")),
        rs.getString(availableColumns.find(_.contains("event_timestamp")).getOrElse("EVENT_TIMESTAMP"))
      )
    }.toList

  // Convert list of rows into a DataFrame
  var masterTable1DF = spark.createDataFrame(
    spark.sparkContext.parallelize(rows_master1),
    schema_master1
  )

  // Automatically normalize all column names to lowercase
  masterTable1DF = masterTable1DF.toDF(masterTable1DF.columns.map(_.toLowerCase): _*)

  // Format event_timestamp column dynamically (if it exists)
  if (masterTable1DF.columns.contains("event_timestamp")) {
    masterTable1DF = masterTable1DF.withColumn(
      "formatted_event_timestamp",
      date_format(col("event_timestamp"), "yyyy-MM-dd HH:mm:ss") // Adjust format as needed
    )
  }

  // Show final DataFrame
  masterTable1DF.show()
} else {
  println("No data found in masterTable1Query!")
}
