// Read tables into DataFrames
val alertLoadDF = spark.read.format("jdbc").option("dbtable", "alert_load").load()
val etlInfoDF = spark.read.format("jdbc").option("dbtable", "etl_info").load()

// Perform left join and filter alerts that are in alert_load but not in etl_info
val unexpectedAlertsDF = alertLoadDF.join(etlInfoDF, Seq("alert_code"), "left_anti")

// Collect unexpected alerts
val unexpectedAlerts = unexpectedAlertsDF.collect()

// Send email if there are unexpected alerts
if (unexpectedAlerts.nonEmpty) {
    val unexpectedAlertCodes = unexpectedAlerts.map(_.getAs[String]("alert_code")).mkString(", ")
    val emailBody = s"The following alerts are in alert_load but missing in etl_info: $unexpectedAlertCodes"
    
    sendEmail("Unexpected Alerts Notification", emailBody)
}

// Define sendEmail function (modify based on your email setup)
def sendEmail(subject: String, body: String): Unit = {
    // Your email logic here
}




import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.Row
import org.apache.spark.sql.types._

val spark: SparkSession = SparkSession.builder().getOrCreate() // Ensure SparkSession is available

// Convert combinedReceivedAlerts to a proper sequence
val combinedReceivedAlertsSeq: Seq[String] = combinedReceivedAlerts.toSeq.map(_.toString)

// Convert to DataFrame using an explicit schema
val schema = StructType(Array(StructField("alert_code", StringType, nullable = false)))
val combinedReceivedAlertsDF = spark.createDataFrame(
    spark.sparkContext.parallelize(combinedReceivedAlertsSeq.map(Row(_))),
    schema
)



option("autoReconnect", "true")
.option("queryTimeout", "1800")   // 30 min timeout
.option("socketTimeout", "600000") // 10 min socket timeout


val alertCount = df.count()
val optimalPartitions = if (alertCount < 50000) 4 else if (alertCount < 200000) 8 else 16

val partitionedDF = df.repartition(optimalPartitions)

partitionedDF.foreachPartition { partition =>
  val parallelProcessing = partition.toList.par  
  parallelProcessing.foreach { row =>
    processAlert(row)
  }
}



df.repartition(optimalPartitions).foreachPartition { partition =>
  val parallelProcessing = partition.toList.par  // Convert to parallel collection
  parallelProcessing.foreach { row =>
    processAlert(row)  // Process each alert in parallel
  }
}


import org.apache.spark.sql.{DataFrame, SparkSession}
import scala.util.Try
import java.io.{BufferedWriter, FileWriter}

// Initialize Spark Session
val spark = SparkSession.builder()
  .appName("Dynamic Timeout Handling")
  .master("yarn")  // Adjust based on your deployment
  .getOrCreate()

// JDBC Configurations
val jdbcUrl = "jdbc:teradata://your_teradata_server"
val jdbcUser = "your_user"
val jdbcPassword = "your_password"
val jdbcDriver = "com.teradata.jdbc.TeraDriver"

// Read from Teradata with Parallelism
val df = spark.read
  .format("jdbc")
  .option("url", jdbcUrl)
  .option("dbtable", "U_DSV_001_RIS1.RAMP_ALERT_LOAD")
  .option("user", jdbcUser)
  .option("password", jdbcPassword)
  .option("driver", jdbcDriver)
  .option("fetchsize", "5000")  // Reduce fetch size for stability
  .option("numPartitions", "8")  // Enable parallel reads
  .option("partitionColumn", "alert_id")
  .option("lowerBound", "1")
  .option("upperBound", "100000")
  .option("queryTimeout", "600")  // Initial timeout (dynamically adjusted)
  .option("socketTimeout", "120000")
  .load()
  .repartition(10)  // Balanced parallelism

// Define Paths for CSV Outputs
val successCsvPath = "/mnt/data/success_alerts.csv"
val failedCsvPath = "/mnt/data/failed_alerts.csv"

// Initialize BufferedWriter for writing CSV
def writeToCSV(filePath: String, content: String): Unit = {
  val writer = new BufferedWriter(new FileWriter(filePath, true))  // Append mode
  writer.write(content + "\n")
  writer.close()
}

// Process Alerts with Dynamic Timeout Handling
df.foreachPartition { partition =>
  partition.foreach { row =>
    val alertCode = row.getAs[String]("alert_code")
    
    var retries = 0
    var success = false
    var dynamicTimeout = 60000  // Start with 60 seconds timeout

    while (retries < 3 && !success) {
      val startTime = System.currentTimeMillis()

      Try {
        println(s"Processing Alert: $alertCode, Attempt: ${retries + 1}, Timeout: $dynamicTimeout ms")

        // Simulate Processing (Replace with actual logic)
        Thread.sleep(scala.util.Random.nextInt(3000)) // Random delay for testing

        success = true  // Mark as success if no exception occurs
      } recover {
        case e: Exception =>
          val elapsedTime = System.currentTimeMillis() - startTime
          println(s"Alert $alertCode failed, Time taken: $elapsedTime ms, Error: ${e.getMessage}")

          if (elapsedTime > dynamicTimeout) {
            println(s"Increasing timeout for $alertCode from $dynamicTimeout to ${dynamicTimeout * 2} ms")
            dynamicTimeout *= 2  // Dynamically increase timeout
          }

          retries += 1
          Thread.sleep(5000)  // Wait before retrying
      }
    }

    // Writing to CSV based on success or failure
    if (success) {
      writeToCSV(successCsvPath, s"$alertCode, SUCCESS")
    } else {
      writeToCSV(failedCsvPath, s"$alertCode, FAILED")
      println(s"Alert $alertCode failed after 3 attempts.")
    }
  }
}

// Ensure a Single CSV File for Each Output
spark.read.text(successCsvPath).coalesce(1).write.mode("overwrite").csv("/mnt/data/final_success.csv")
spark.read.text(failedCsvPath).coalesce(1).write.mode("overwrite").csv("/mnt/data/final_failed.csv")

println("Processing completed. Success and failure alerts saved.")
