import org.apache.hadoop.fs.{FileSystem, Path, FileStatus}
import org.apache.spark.sql.SparkSession
import java.net.URI
import java.text.SimpleDateFormat
import java.util.Date
import java.io.File

object HdfsToLocalRecursiveCopy {
  def copyHdfsToLocal(hdfs: FileSystem, hdfsPath: Path, localPath: File): Unit = {
    if (!localPath.exists()) localPath.mkdirs() // Create local directory if not exists

    val fileStatuses: Array[FileStatus] = hdfs.listStatus(hdfsPath)

    fileStatuses.foreach { fileStatus =>
      val hdfsFilePath = fileStatus.getPath
      val localFile = new File(localPath, hdfsFilePath.getName)

      if (fileStatus.isFile) { // If it's a file, copy it directly
        val localFilePath = new Path(localFile.getAbsolutePath)
        println(s"ðŸ“‚ Copying file: $hdfsFilePath -> $localFilePath")
        hdfs.copyToLocalFile(false, hdfsFilePath, localFilePath)
      } else if (fileStatus.isDirectory) { // If it's a folder, recurse into it
        println(s"ðŸ“ Creating local directory: $localFile")
        localFile.mkdirs()
        copyHdfsToLocal(hdfs, hdfsFilePath, localFile) // Recursive call for subdirectories
      }
    }
  }

  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder().appName("HDFS to Local Recursive Copy").getOrCreate()
    val hadoopConf = spark.sparkContext.hadoopConfiguration
    val hdfs = FileSystem.get(new URI("hdfs://nameservice1"), hadoopConf)

    // Generate a dynamic local folder
    val timestamp = new SimpleDateFormat("yyyyMMddHHmmss").format(new Date())
    val hdfsSourcePath = new Path("hdfs://nameservice1/tmp/ramp/2025021118") // HDFS Source
    val localTargetPath = new File(s"/tmp/hdfs_output/hdfs_copy_$timestamp") // Local Destination

    println(s"ðŸš€ Copying from HDFS: $hdfsSourcePath -> Local: $localTargetPath")
    copyHdfsToLocal(hdfs, hdfsSourcePath, localTargetPath) // Start Copy
    println(s"âœ… Copy completed successfully to: $localTargetPath")

    spark.stop()
  }
}
