import org.apache.hadoop.fs.{FileSystem, Path}
import org.apache.spark.sql.SparkSession
import java.net.URI
import java.text.SimpleDateFormat
import java.util.Date

object HdfsToLocalCopy {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("HDFS to Local Copy")
      .getOrCreate()

    val hadoopConf = spark.sparkContext.hadoopConfiguration
    val hdfs = FileSystem.get(new URI("hdfs://nameservice1"), hadoopConf)
    val localFs = FileSystem.getLocal(hadoopConf)

    // Generate timestamp dynamically
    val timestamp = new SimpleDateFormat("yyyyMMddHHmmss").format(new Date())

    // Define paths
    val hdfsSourcePath = new Path("hdfs://nameservice1/tmp/ramp/2025021118") // Change this
    val localBasePath = "/tmp/hdfs_output/"
    val localTargetPath = new Path(localBasePath + s"hdfs_copy_$timestamp")

    // Ensure local directory exists
    if (!localFs.exists(new Path(localBasePath))) {
      localFs.mkdirs(new Path(localBasePath))
    }
    localFs.mkdirs(localTargetPath)
    println(s"Created local directory: $localTargetPath")

    // Copy from HDFS to local
    hdfs.copyToLocalFile(false, hdfsSourcePath, localTargetPath, true)
    println(s"Successfully copied from HDFS to local: $localTargetPath")

    spark.stop()
  }
}
