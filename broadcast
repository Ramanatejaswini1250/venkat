import org.apache.hadoop.fs.{FileSystem, Path, FileStatus}
import org.apache.spark.sql.SparkSession
import java.net.URI
import java.text.SimpleDateFormat
import java.util.Date
import java.io.File
import org.apache.commons.io.FileUtils

object HdfsToLocalRecursiveCopy {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("HDFS to Local Recursive Copy with FileUtils")
      .getOrCreate()

    val hadoopConf = spark.sparkContext.hadoopConfiguration
    val hdfs = FileSystem.get(new URI("hdfs://nameservice1"), hadoopConf)
    val localFs = FileSystem.getLocal(hadoopConf)

    // Generate timestamp dynamically
    val timestamp = new SimpleDateFormat("yyyyMMddHHmmss").format(new Date())

    // Define paths
    val hdfsSourcePath = new Path("hdfs://nameservice1/tmp/ramp/2025021118")  // HDFS Source Directory
    val localTargetPath = new File(s"/tmp/hdfs_output/hdfs_copy_$timestamp")  // Final Local Directory

    // Create target directory if not exists
    if (!localTargetPath.exists()) localTargetPath.mkdirs()

    println(s"Copying recursively from HDFS: $hdfsSourcePath to Local: $localTargetPath")

    // **Step 1: List all files and directories in the HDFS path**
    val fileStatuses: Array[FileStatus] = hdfs.listStatus(hdfsSourcePath)

    fileStatuses.foreach { fileStatus =>
      val hdfsFilePath = fileStatus.getPath
      val localFilePath = new Path(localTargetPath.getAbsolutePath, hdfsFilePath.getName)

      // **Step 2: Copy Each File Individually**
      if (fileStatus.isFile) {
        println(s"Copying file: $hdfsFilePath -> $localFilePath")
        hdfs.copyToLocalFile(false, hdfsFilePath, localFilePath)
      } else {
        println(s"Skipping directory (handled separately): $hdfsFilePath")
      }
    }

    println(s"âœ… All files copied successfully to: $localTargetPath")

    spark.stop()
  }
}
