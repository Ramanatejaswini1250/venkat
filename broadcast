import org.apache.spark.sql.SparkSession
import org.apache.hadoop.fs.{FileSystem, Path, FileStatus}
import java.io.File

object HdfsToLocalMirror {
  def main(args: Array[String]): Unit = {
    // Step 1: Initialize SparkSession
    val spark = SparkSession.builder()
      .appName("HDFS to Local Mirror")
      .master("local[*]") // Adjust as necessary
      .getOrCreate()

    // Step 2: Define HDFS and local paths
    val hdfsDirPath = "hdfs://your-hdfs-path/currenttimestamp_rbsc_we’d"
    val localDirPath = "/local/path/currenttimestamp_rbsc_we’d"

    // Step 3: Create the mirrored local directory
    val localDir = new File(localDirPath)
    if (!localDir.exists()) {
      localDir.mkdirs()
    }

    // Step 4: List all files in the HDFS directory
    val hadoopConf = spark.sparkContext.hadoopConfiguration
    val fs = FileSystem.get(hadoopConf)
    val hdfsPath = new Path(hdfsDirPath)

    // Check if the path exists in HDFS
    if (fs.exists(hdfsPath)) {
      val fileStatusList: Array[FileStatus] = fs.listStatus(hdfsPath)

      // Step 5: Filter and copy only .csv files
      fileStatusList.foreach { fileStatus =>
        if (fileStatus.isFile && fileStatus.getPath.getName.endsWith(".csv")) {
          val hdfsFilePath = fileStatus.getPath
          val localFilePath = new Path(localDirPath + "/" + hdfsFilePath.getName)

          // Correct usage of copyToLocalFile
          fs.copyToLocalFile(false, hdfsFilePath, localFilePath, true)
          println(s"Copied ${hdfsFilePath} to ${localFilePath}")
        }
      }
    } else {
      println(s"HDFS directory $hdfsDirPath does not exist.")
    }

    // Step 6: Stop SparkSession
    spark.stop()
  }
}

