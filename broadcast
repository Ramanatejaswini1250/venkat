import org.apache.spark.sql.functions._
import org.apache.spark.sql.SparkSession
import java.time.LocalDateTime
import java.time.format.DateTimeFormatter

// Initialize SparkSession (if not already done)
val spark = SparkSession.builder.appName("MasterTableTransformation").getOrCreate()

// Define formatters
val inputFormatter = DateTimeFormatter.ofPattern("dd-MM-yyyy hh:mm:ss a")
val outputFormatter = DateTimeFormatter.ofPattern("dd/MM/yyyy HH:mm")

// Function to format DateTime
def formatDateTime(columnName: String) = {
  when(col(columnName).isNotNull, date_format(col(columnName), "dd/MM/yyyy HH:mm"))
    .otherwise(lit("Unknown"))
}

// Apply transformations directly on the DataFrame
val formattedMasterTable1DF = masterTable1DF
  .withColumn("formattedEventTimestamp", formatDateTime("EVENT_TIMESTAMP"))
  .withColumn("formattedAlertDueDate", formatDateTime("ALERT_DUE_DATE"))

// Convert to CSV format by selecting required columns
val formattedMasterTable1CSV = formattedMasterTable1DF
  .selectExpr(
    "getString(1) as col1",
    "getString(2) as col2",
    "getString(3) as col3",
    "formattedEventTimestamp",
    "getString(5) as col5",
    "formattedAlertDueDate"
  )

// Repartition if needed
val singleFileDF = formattedMasterTable1CSV.repartition(1)

// Collect & Write
singleFileDF.collect().foreach { row =>
  master_writer1.write(row.mkString(","))
  master_writer1.newLine()
}
