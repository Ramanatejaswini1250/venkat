import org.apache.spark.sql.{DataFrame, SparkSession}
import scala.util.Try
import java.io.{BufferedWriter, FileWriter}

// Initialize Spark Session
val spark = SparkSession.builder()
  .appName("Dynamic Timeout Handling")
  .master("yarn")  // Adjust based on your deployment
  .getOrCreate()

// JDBC Configurations
val jdbcUrl = "jdbc:teradata://your_teradata_server"
val jdbcUser = "your_user"
val jdbcPassword = "your_password"
val jdbcDriver = "com.teradata.jdbc.TeraDriver"

// Read from Teradata with Parallelism
val df = spark.read
  .format("jdbc")
  .option("url", jdbcUrl)
  .option("dbtable", "U_DSV_001_RIS1.RAMP_ALERT_LOAD")
  .option("user", jdbcUser)
  .option("password", jdbcPassword)
  .option("driver", jdbcDriver)
  .option("fetchsize", "5000")  // Reduce fetch size for stability
  .option("numPartitions", "8")  // Enable parallel reads
  .option("partitionColumn", "alert_id")
  .option("lowerBound", "1")
  .option("upperBound", "100000")
  .option("queryTimeout", "600")  // Initial timeout (dynamically adjusted)
  .option("socketTimeout", "120000")
  .load()
  .repartition(10)  // Balanced parallelism

// Define Paths for CSV Outputs
val successCsvPath = "/mnt/data/success_alerts.csv"
val failedCsvPath = "/mnt/data/failed_alerts.csv"

// Initialize BufferedWriter for writing CSV
def writeToCSV(filePath: String, content: String): Unit = {
  val writer = new BufferedWriter(new FileWriter(filePath, true))  // Append mode
  writer.write(content + "\n")
  writer.close()
}

// Process Alerts with Dynamic Timeout Handling
df.foreachPartition { partition =>
  partition.foreach { row =>
    val alertCode = row.getAs[String]("alert_code")
    
    var retries = 0
    var success = false
    var dynamicTimeout = 60000  // Start with 60 seconds timeout

    while (retries < 3 && !success) {
      val startTime = System.currentTimeMillis()

      Try {
        println(s"Processing Alert: $alertCode, Attempt: ${retries + 1}, Timeout: $dynamicTimeout ms")

        // Simulate Processing (Replace with actual logic)
        Thread.sleep(scala.util.Random.nextInt(3000)) // Random delay for testing

        success = true  // Mark as success if no exception occurs
      } recover {
        case e: Exception =>
          val elapsedTime = System.currentTimeMillis() - startTime
          println(s"Alert $alertCode failed, Time taken: $elapsedTime ms, Error: ${e.getMessage}")

          if (elapsedTime > dynamicTimeout) {
            println(s"Increasing timeout for $alertCode from $dynamicTimeout to ${dynamicTimeout * 2} ms")
            dynamicTimeout *= 2  // Dynamically increase timeout
          }

          retries += 1
          Thread.sleep(5000)  // Wait before retrying
      }
    }

    // Writing to CSV based on success or failure
    if (success) {
      writeToCSV(successCsvPath, s"$alertCode, SUCCESS")
    } else {
      writeToCSV(failedCsvPath, s"$alertCode, FAILED")
      println(s"Alert $alertCode failed after 3 attempts.")
    }
  }
}

// Ensure a Single CSV File for Each Output
spark.read.text(successCsvPath).coalesce(1).write.mode("overwrite").csv("/mnt/data/final_success.csv")
spark.read.text(failedCsvPath).coalesce(1).write.mode("overwrite").csv("/mnt/data/final_failed.csv")

println("Processing completed. Success and failure alerts saved.")
