import org.apache.spark.sql.{Row, SparkSession}
import org.apache.spark.sql.types._
import scala.collection.mutable.ArrayBuffer

// Define schema with correct data types
val schema_master1 = StructType(Seq(
  StructField("alert_id", StringType, nullable = true),
  StructField("alert_code", StringType, nullable = true),
  StructField("business_line", StringType, nullable = true),
  StructField("event_timestamp", StringType, nullable = true),
  StructField("priority", IntegerType, nullable = true),  // Int must be handled safely
  StructField("alert_due_date", StringType, nullable = true)
))

// Collect rows
val rows_master1 = ArrayBuffer[Row]()

while (masterTable1.next()) {
  val alertId = Option(masterTable1.getString("alert_id")).orNull
  val alertCode = Option(masterTable1.getString("alert_code")).orNull
  val businessLine = Option(masterTable1.getString("business_line")).orNull
  val eventTimestamp = Option(masterTable1.getString("event_timestamp")).orNull
  val alertDueDate = Option(masterTable1.getString("alert_due_date")).orNull

  // Handle priority safely
  val priority = try {
    if (masterTable1.getObject("priority") != null) {
      Some(masterTable1.getInt("priority")) // Avoid NullPointerException
    } else None
  } catch {
    case _: Exception => None
  }

  // Create row
  val row = Row(alertId, alertCode, businessLine, eventTimestamp, priority.getOrElse(null), alertDueDate)
  
  rows_master1 += row
}

// Debugging: Print rows before creating DataFrame
println(s"Schema: $schema_master1")
println(s"Row Count: ${rows_master1.size}")
rows_master1.foreach(row => println(s"Row Data: $row"))

// Convert to DataFrame
val masterTable1DF = spark.createDataFrame(
  spark.sparkContext.parallelize(rows_master1.toSeq),  // Convert ArrayBuffer to Seq
  schema_master1
)

// Show the DataFrame
masterTable1DF.show()
