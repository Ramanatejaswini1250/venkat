import org.apache.spark.sql.{SparkSession, DataFrame}
import scala.util.{Try, Success, Failure}

def waitForDataToLoadAndValidate(
  spark: SparkSession,
  alertCode: String,
  countCheckQuery: String,
  jdbcUrl: String,
  jdbcUser: String,
  jdbcPassword: String,
  jdbcDriver: String,
  maxRetries: Int = 10,
  retryInterval: Long = 10000
): Boolean = {

  var retries = 0
  var dataLoaded = false
  var dtCount: Long = 0 // Define dtCount outside the loop

  while (retries < maxRetries && !dataLoaded) {
    val queryWithAlertCode = countCheckQuery.replace("$alertCode", s"'$alertCode'")

    try {
      println(s"[Attempt ${retries + 1}] Executing Query: $queryWithAlertCode")

      val countDF = spark.read
        .format("jdbc")
        .option("url", jdbcUrl)
        .option("dbtable", s"($queryWithAlertCode) AS subquery")
        .option("user", jdbcUser)
        .option("password", jdbcPassword)
        .option("driver", jdbcDriver)
        .load()

      // Extract dtCount safely
      dtCount = Try {
        val row = countDF.collect().headOption
        row.map(_.getAs[Long]("count")).getOrElse(0L)
      }.getOrElse(0L)

      println(s"Retrieved dtCount: $dtCount")

      if (dtCount > 0) {
        println(s"✅ Data has been successfully loaded for alertCode: $alertCode.")
        dataLoaded = true
      } else {
        println(s"❌ Data not available yet for alertCode: $alertCode. Waiting for ${retryInterval / 1000} seconds before retrying...")
        retries += 1
        Thread.sleep(retryInterval) // Wait before retrying
      }
      
    } catch {
      case e: Exception =>
        println(s"⚠️ Error executing query: ${e.getMessage}")
        e.printStackTrace()
    }
  }

  if (!dataLoaded) {
    println(s"⛔ Data for alertCode: $alertCode was NOT loaded after $maxRetries retries.")
  }

  println(s"Final dtCount: $dtCount") // Print dtCount outside the loop for debugging
  dataLoaded
}
