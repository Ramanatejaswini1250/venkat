import java.nio.file.{Files, Paths}
import java.sql.{Connection, Statement}
import org.apache.spark.sql.{DataFrame, SparkSession}
import scala.collection.mutable.ListBuffer

def executeWithSpark(df: DataFrame, spark: SparkSession): Unit = {
  import spark.implicits._

  val failedQueriesDF = df.mapPartitions { partition =>
    val conn_rss = getConnection()
    val stmt_rss = conn_rss.createStatement()

    val failedQueries = ListBuffer[String]() // Collect failed queries per partition

    try {
      partition.foreach { row =>
        val sqlFilePath = row.getAs[String]("bteqlocation") // Assuming column name `bteqlocation`
        
        if (Files.exists(Paths.get(sqlFilePath))) {
          val (_, _, failed) = runSqlScript(conn_rss, stmt_rss, sqlFilePath)
          failedQueries ++= failed
        } else {
          val missingFileMessage = s"ðŸš¨ SQL file not found: $sqlFilePath"
          failedQueries += missingFileMessage
          println(missingFileMessage)
        }
      }
    } finally {
      stmt_rss.close()
      conn_rss.close()
    }

    failedQueries.toIterator
  }.toDF("failed_query")

  // Collect all failures and send an email if needed
  val allFailedQueries = failedQueriesDF.collect().map(_.getString(0))

  if (allFailedQueries.nonEmpty) {
    val failureReport = allFailedQueries.mkString("\n")
    println(s"ðŸš¨ SQL Failures Detected. Sending Email...\n$failureReport")

    sendEmailNotification(
      "SQL Execution Failed",
      s"Failed Queries:\n$failureReport",
      "CDAORiskAlertDeliveryTeam@cba.com.au"
    )
  } else {
    println("âœ… All SQL queries executed successfully.")
  }
}
