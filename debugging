import java.sql.{Connection, Statement}
import java.util.concurrent.ConcurrentLinkedQueue
import java.io.{BufferedWriter, File, FileWriter}
import org.apache.spark.sql.{DataFrame, SparkSession}
import scala.collection.mutable.ListBuffer
import scala.io.Source

object RampAutomationExecution {

  def runSqlScript(
      conn_rss: Connection,
      stmt_rss: Statement,
      scriptPath: String,
      failedSqlQueue: ConcurrentLinkedQueue[String],
      partitionId: Int
  ): Unit = {

    println(s"Running SQL script from: $scriptPath on Partition: $partitionId")

    var currentCommand: Option[String] = None
    val failedQueries = ListBuffer[String]() // Track failed queries

    try {
      conn_rss.setAutoCommit(false)
      stmt_rss.setQueryTimeout(30)

      println("Reading SQL script...")
      val scriptContent = readFileAsString(scriptPath)
      val scriptWithoutComments = removeBlockComments(scriptContent)

      val lines = scriptWithoutComments.split("\n").toList
      val validLines = lines
        .drop(4)
        .dropRight(4)
        .map(removeInlineComments)
        .filterNot(line =>
          line.trim.toUpperCase.contains("IF ERRORCODE <> 0 THEN .GOTO ERROR;".toUpperCase)
        )
        .filter(_.nonEmpty)

      // DELETE existing data before loading new data
      val masterTables = extractMasterTables(validLines)
      masterTables.foreach { table =>
        val deleteStmt = s"DELETE FROM $table;"
        println(s"Executing: $deleteStmt")
        stmt_rss.execute(deleteStmt)
      }

      val remainingCommands = validLines.mkString("\n").split(";").map(_.trim).filter(_.nonEmpty)
      for (query <- remainingCommands) {
        currentCommand = Some(query)
        println(s"Executing: $query")
        try {
          stmt_rss.execute(query)
        } catch {
          case e: Exception =>
            val failedCommand = currentCommand.getOrElse("Unknown Command")
            failedQueries += s"âŒ SQL Failed: $failedCommand. Error: ${e.getMessage}"
            failedSqlQueue.add(failedCommand) // Store failed query in queue
            conn_rss.rollback() // Rollback on failure
        }
      }

      conn_rss.commit()

    } catch {
      case ex: Exception =>
        val failedCommand = currentCommand.getOrElse("Unknown Command")
        failedQueries += s"âŒ Critical Failure: $failedCommand. Error: ${ex.getMessage}"
        failedSqlQueue.add(failedCommand)
        conn_rss.rollback()
    } finally {
      conn_rss.setAutoCommit(true)

      // Save failed queries to a temp file (one per partition)
      if (failedQueries.nonEmpty) {
        val filePath = s"/tmp/failed_queries_$partitionId.txt"
        val file = new File(filePath)
        val bw = new BufferedWriter(new FileWriter(file, true))
        failedQueries.foreach(query => bw.write(query + "\n"))
        bw.close()
      }
    }
  }

  def executeWithSpark(df: DataFrame, spark: SparkSession): Unit = {
    val failedSqlQueue = new ConcurrentLinkedQueue[String]()
    val broadcastFailedSqlQueue = spark.sparkContext.broadcast(failedSqlQueue)

    df.foreachPartition { partition =>
      val partitionId = TaskContext.getPartitionId() // Get partition ID
      val conn_rss = getConnection()
      val stmt_rss = conn_rss.createStatement()
      try {
        runSqlScript(conn_rss, stmt_rss, "your_script_path", broadcastFailedSqlQueue.value, partitionId)
      } finally {
        stmt_rss.close()
        conn_rss.close()
      }
    }

    // Collect all failed queries from temp files
    val allFailedQueries = spark.sparkContext
      .parallelize(new File("/tmp").listFiles().filter(_.getName.startsWith("failed_queries_")))
      .flatMap(file => Source.fromFile(file).getLines())
      .distinct()
      .collect()

    // Send email if any failures
    if (allFailedQueries.nonEmpty) {
      val failureReport = allFailedQueries.mkString("\n")
      println(s"ðŸš¨ SQL Failures Detected. Sending Email...\n$failureReport")

      sendEmailNotification(
        "SQL Execution Failed",
        s"Failed Queries:\n$failureReport",
        "CDAORiskAlertDeliveryTeam@cba.com.au"
      )
    }
  }
}
