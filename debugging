import java.sql.{Connection, Statement}
import org.apache.spark.util.LongAccumulator
import scala.collection.mutable.ListBuffer

object RampAutomationExecution {
  
  def runSqlScript(
      conn_rss: Connection,
      stmt_rss: Statement,
      scriptPath: String,
      failedSqlAccumulator: LongAccumulator
  ): (String, String) = {

    println(s"Running SQL script from: $scriptPath")

    var currentCommand: Option[String] = None
    val executedQueries = ListBuffer[String]() // Track executed queries
    val failedQueries = ListBuffer[String]()   // Track failed queries

    try {
      conn_rss.setAutoCommit(false)
      stmt_rss.setQueryTimeout(30) // Set query timeout to 30 seconds

      println("Reading SQL script...")
      val scriptContent = readFileAsString(scriptPath)
      val scriptWithoutComments = removeBlockComments(scriptContent)

      val lines = scriptWithoutComments.split("\n").toList

      // Process SQL script: remove first & last 4 lines, inline comments, and unnecessary lines
      val validLines = lines
        .drop(4)
        .dropRight(4)
        .map(removeInlineComments)
        .filterNot(line =>
          line.trim.toUpperCase.contains("IF ERRORCODE <> 0 THEN .GOTO ERROR;".toUpperCase)
        )
        .filter(_.nonEmpty)

      // Extract master tables dynamically
      val masterTables = extractMasterTables(validLines)
      println(s"Extracted master tables: ${masterTables.mkString(", ")}")

      val masterTable1 = if (masterTables.nonEmpty) masterTables.head else "default_mastertable1"
      val masterTable2 = if (masterTables.length > 1) masterTables(1) else "default_mastertable2"

      // DELETE existing data from master tables before loading new data
      masterTables.foreach { table =>
        val deleteStmt = s"DELETE FROM $table;"
        println(s"Executing: $deleteStmt")
        stmt_rss.execute(deleteStmt)
        executedQueries += deleteStmt
      }

      // Execute remaining SQL queries
      val remainingCommands = validLines.mkString("\n").split(";").map(_.trim).filter(_.nonEmpty)
      for (query <- remainingCommands) {
        currentCommand = Some(query) // Track the current command

        // Transform INSERT INTO queries to include column names if needed
        val transformedQuery = if (query.toUpperCase.startsWith("INSERT INTO")) {
          transformInsertToIncludeColumns(query, stmt_rss)
        } else {
          query
        }

        println(s"Executing: $transformedQuery")
        try {
          stmt_rss.execute(transformedQuery)
          executedQueries += transformedQuery
        } catch {
          case e: Exception =>
            val failedCommand = currentCommand.getOrElse("Unknown Command")
            failedQueries += failedCommand
            failedSqlAccumulator.add(1) // Track failure in accumulator

            println(s"Failed to execute query: $failedCommand. Error: ${e.getMessage}")

            conn_rss.rollback() // Rollback on failure
            throw new RuntimeException(s"SQL Execution Failed: $failedCommand. ${e.getMessage}")
        }
      }

      conn_rss.commit() // Commit only if all queries run successfully
      println("SQL script executed successfully.")

      (masterTable1, masterTable2) // Return master tables
    } catch {
      case ex: Exception =>
        ex.printStackTrace()
        val failedCommand = currentCommand.getOrElse("Unknown Command")
        failedQueries += failedCommand
        failedSqlAccumulator.add(1) // Track failure in accumulator

        println(s"Critical Error: $failedCommand. ${ex.getMessage}")

        conn_rss.rollback() // Rollback on failure
        throw new RuntimeException(s"SQL Execution Failed: $failedCommand. ${ex.getMessage}")
    } finally {
      conn_rss.setAutoCommit(true) // Reset auto-commit mode
    }
  }

  def executeWithSpark(df: org.apache.spark.sql.DataFrame, spark: org.apache.spark.sql.SparkSession): Unit = {
    // Accumulator to track failed queries
    val failedSqlAccumulator = spark.sparkContext.longAccumulator("FailedSQLCount")

    df.foreachPartition { partition =>
      val conn_rss = getConnection() // Get DB connection
      val stmt_rss = conn_rss.createStatement()
      try {
        runSqlScript(conn_rss, stmt_rss, "your_script_path", failedSqlAccumulator) // Run SQL
      } finally {
        stmt_rss.close()
        conn_rss.close() // Close connection
      }
    }

    // **After foreachPartition completes, check accumulator and send email if failures occurred**
    if (failedSqlAccumulator.value > 0) {
      println(s"Detected ${failedSqlAccumulator.value} SQL failures. Sending failure email.")
      sendEmailNotification(
        "SQL Execution Failed",
        s"${failedSqlAccumulator.value} queries failed during execution.",
        "CDAORiskAlertDeliveryTeam@cba.com.au"
      )
    } else {
      println("All SQL queries executed successfully.")
    }
  }
}
