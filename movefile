import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, FileStatus, Path}
import org.apache.spark.sql.{SparkSession, DataFrame}

object RampAutomationExecution {
  def main(args: Array[String]): Unit = {
    // Initialize Spark session
    val spark = SparkSession.builder()
      .appName("RampAutomationExecution")
      .getOrCreate()

    import spark.implicits._

    // Sample DataFrame (replace with your actual DataFrame)
    val formattedMasterTable1DF = Seq(
      ("data1"), ("data2")
    ).toDF("column1")

    // Variables for dynamic paths
    val currentTime = System.currentTimeMillis()
    val currentDay = "20240130" // Replace with dynamic value

    // HDFS and local destination paths
    val hdfsOutputPath = s"hdfs:///tmp/ramp/${currentTime}_RBSCSS_${currentDay}"
    val finalDestinationPath = s"/disk/source/${currentTime}_RBSCSS_${currentDay}/master1.csv"

    // Print the paths
    println(s"Temporary HDFS Path: $hdfsOutputPath")

    // Write DataFrame to HDFS in CSV format (allow multiple part files)
    formattedMasterTable1DF
      .write
      .option("header", "true")
      .option("delimiter", ",")
      .mode("overwrite")
      .csv(hdfsOutputPath)

    println(s"CSV Part files written to: $hdfsOutputPath")

    // HDFS file system setup - Initialize FileSystem and Configuration outside of transformations
    val conf = new Configuration()
    val fs = FileSystem.get(conf) // Initialize FileSystem instance (should be outside Spark transformations)

    // Efficient file listing using globStatus
    val partFiles: Array[FileStatus] = fs.globStatus(new Path(s"$hdfsOutputPath/part*"))

    // Check if part files are available
    if (partFiles.nonEmpty) {
      // Read all part files into a single DataFrame
      val partFilesPaths = partFiles.map(_.getPath.toString)
      val df = spark.read
        .option("header", "true")
        .csv(partFilesPaths: _*)  // Reading all part files into a single DataFrame

      // Repartitioning to allow parallel processing and improve performance
      val mergedDF = df.repartition(10)  // Adjust this number based on your dataset size

      // Write the merged DataFrame to a single output file
      mergedDF
        .write
        .option("header", "true")
        .option("delimiter", ",")
        .mode("overwrite")
        .csv(finalDestinationPath)

      println(s"Merged CSV file written to: $finalDestinationPath")

      // Clean up part files from the HDFS directory (optional)
      // fs.delete(new Path(hdfsOutputPath), true)
    } else {
      println("No part files found. Something went wrong!")
    }

    // Stop the Spark session
    spark.stop()
  }
}
